#+title: Breathing New Life Into Existing Visualizations A Natural Language Driven Manipulation Framework

https://arxiv.org/pdf/2404.06039

- Four levels
- Seven types
- Some works are about creating descriptions to explain the chart. Others work by highlighting the important parts. Others by creating labels, titles, etc.
- Referecences 37 and 41 talk about the command-based. It looks to be well solved.

---

**We propose a method that enables manipulating existing visualizations to respond to natural language queries, regardless of the visualizations’ original design and implementation**. We aim to augment existing visualizations by seamlessly integrating interactivity implementing dynamic manipulations and re-encoding strategies to adapt to a wide array of visualization tasks. To tackle the aforementioned challenges, we undertake the collection and organization of potential natural language inquiries related to visualizations. Additionally, **we propose a design space for representing visualization-related tasks**. Within this framework, we introduce a deep learning-based natural language-to-task translator (NL-task translator) specifically engineered to **parse natural language queries into structured and hierarchical task descriptions**. **To train the NL-Task translator, we leverage large-scale language models (such as GPT3.5) to assist in curating a diverse cross-domain dataset comprising natural language expressions and their associated tasks**. **Then, we use this dataset to train a smaller model, balancing affordability and accuracy**. **This process can be seen as a form of knowledge distillation, where the knowledge of the large model is used to guide the learning of the smaller model**. **Once we have successfully extracted hierarchical tasks, we proceed to translate them into concrete visualization manipulations**. Furthermore, **we define a visualization manipulation space encompassing common visualization types, such as bar charts, line charts, and area charts. This visualization manipulation space encompasses four levels and seven types**. The manipulations support dynamically transforming the visual elements of visualizations, aligning them with users’ exploration requirements. **By introducing the idea of using a large LLM to assist in dataset construction and training a small LLM, we can reduce the scale and computational overhead of the model while ensuring its performance, making it more cost-effective**. This method combines the advantages of knowledge engineering and data-driven approaches, providing a feasible solution for natural language-driven visualization interaction. The contributions of this paper can be summarized as follows:

- We proposed a deep learning-based natural language-to-task translator that supports parsing users’ natural language queries about visualizations into structured-format tasks.
- We curated a dataset for natural language-to-visualization tasks, covering various domains, diverse tasks, and varied natural language expressions.
- We proposed a manipulation space for common visualizations and a method for converting visual tasks into a series of visualization manipulations.

Our work provides a natural language interface for performing manipulations on visualizations, which relates to **natural language interfaces, visualization tasks, and visualization manipulations**.

While the approaches mentioned above focus on constructing visualizations from data, some methods [15–18, 33] focused on existing visualizations. Eviza [33] and Evizeon [15] convert natural language input into filters applied to visualizations. Kim et al. [17] generate explanations to answer questions related to existing visualization charts. Some methods aim to generate natural language content for existing visualizations, for example, generate description [21] and title [19]. Instead of producing explanations, Lai et al. [18] emphasize highlighting charts to help users better understand them. Existing approaches mainly concentrate on either QA systems based on visualizations or the construction of visualizations from known data and programming visualization. In this work, we propose a method that focuses on manipulating existing visualizations that do not depend on underlying data or implementation methods. Moreover, the natural language queries in this method are not limited to command-based language; instead, they are grounded in diverse, task-oriented questions.

Brehmer and Munzner [6] classify visualization tasks on multiple levels. The "why" level involves users searching for elements of interest (corresponding to data items) and querying on these data items. Queries can include identifying, comparing, and summarizing. Amar et al. [1] provides a more detailed, low-level classification by summarizing ten tasks: retrieving a value, filtering, computing derived values, finding extrema, sorting, determining a range, characterizing distribution, finding anomalies, clustering, and correlating. NL4DV [28] also classifies tasks into several types. Articulate [40] classifies natural language words into eight task categories, including comparison, relationship, composition, distribution, statistics, manipulation, and time series. Based on Amar et al.’s taxonomy [1], Fu et al. [10] construct a natural language utterances tasks classification. Compared to these works, our work focuses on the hierarchical structure of tasks in natural language, such as filtering being a step in value retrieval.

**We aim to enable manipulations of static visualization to answer natural language queries. Command-based natural language queries [37, 41] have been previously addressed. Our approach focuses on tasks-based natural language, allowing users to express their intended tasks without specifying the specific changes to visual elements in a visualization. For instance, a command-based instruction might be “Sort the countries in the axis according to their heights,” while a task-based instruction might be “What is the country with the third highest GDP?” The latter is more user-friendly as it directly conveys users’ thoughts, while the former requires users to have certain visualization expertise.**

The advancement of large language models (LLMs) has greatly simplified the process of extracting structured information from natural language. **Our goal is to enable lightweight deployment of this process on local computers**. To achieve this, we propose a knowledge distillation approach that combines our domain knowledge with the capabilities of large-size LLMs to curate datasets and fine-tune a smaller LLM. This method allows us to balance accuracy and cost-effectiveness.

Fig. 2. Natural language queries in the context of visualization are transformed into nested high-level tasks, which are represented through a series of visualization manipulations. Visualization manipulations parse tasks from bottom to top, beginning with the resolution of filtering conditions and followed by comparative tasks. **Different tasks are represented through the combination of visualization manipulations such as highlighting, annotation, reordering, or remapping**.

Visualization tasks can be classified into three categories: identification, comparison, and summarization [6]. Identification involves finding data items based on known indices or attributes. Comparison involves comparing multiple groups of data items. Summarization involves obtaining overall insights from the visualization. These three categories reflect the users’ high-level goals, but they omit the low-level manipulations that users execute to accomplish these goals. High-level tasks such as identification, comparison, and summarization correspond to the users’ primary intentions, but the natural language tasks often exhibit more complexity than this simple classification implies. For instance, identification might entail applying filters or deriving new attributes from existing filters. The comparison might necessitate selecting comparable entities and defining common or distinct filters. Summarization might demand aggregating or sorting data items according to some criteria.

**The hierarchical structure of visualization tasks can thus be constructed utilizing the following fundamental operations**.

- **Filtering** is the process of reducing the number of focused data items by restricting the range of visual elements based on certain conditions. This can be accomplished by selecting from a categorical list or specifying a range of ordinal or temporal values. The general form of filtering is {attr: "Name", op: "Op", value: "Value"}, for example, {attr: "time", op: "=", value: "2000"} denotes selecting data items with the attribute time equal to 2000.
- **Identification**: An instance of identification is “What is the price of apples in 2022”. The specific identification information pertains to lower-level operations and is recorded in filter.
- **Comparison**: An instance of comparison is “What is the difference in price between apples and oranges in 2022?”, which includes two identifications for apples and oranges in 2022 respectively. The sub-attributes of the comparison task are two identification tasks, denoting the objects of comparison.
- **Aggregation**: Aggregation includes max, min, and average to generate a value from a list of data, which can serve as a filter value or an identified value. To determine aggregation, two components are needed: the attribute and the type of aggregation. For instance, to find countries with a life expectancy higher than the average, the syntax of aggregation is {aggregate: "avg", attribute: "life expectancy"}.
- **Derivation**: The form of derivation generates new attributes according to original attributes, for example, generates rank according to a certain quantitative attribute

  The primary goal of the constructed dataset is to train a model capable of handling data attributes across diverse domains and addressing various tasks posed by different users. To achieve this, we create a training dataset to ensure diversity in data attributes, tasks, and natural language.

- R1. Task diversity necessitates that the dataset encompasses operations related to various tasks such as identification, comparison, and summarization.
- R2. Attribute diversity aims to support visualizations across multiple domains, as the natural language employed to describe attributes from distinct domains may vary significantly. For instance, the language used to describe fruit prices may differ from that used to discuss daily new cases of specific diseases.
- R3. Visual channel diversity means allowing users to specify visual elements beyond just data attributes in visualizations, such as color, orientation, and shape.
- R4. Natural language diversity is crucial for ensuring the model’s generalization across different scenarios. As users may have diverse presentation preferences and use various forms of natural language to convey identical meanings, accommodating this diversity in the dataset will improve the model’s applicability to a broader range of users and situations

  Table 1. Various combinations of attribute types in visualization charts along with their respective natural language query tasks and templates. These tasks are broadly categorized into four types, namely identification, comparison, trend analysis, and summation. In the table, the quantitative, categorical, and temporal attributes are referred to as 𝑄𝑛𝑎𝑚𝑒, 𝐶𝑛𝑎𝑚𝑒, and 𝑇𝑛𝑎𝑚𝑒, respectively. The value choices for these attributes are denoted as 𝑄𝑖 , 𝐶𝑖 , and 𝑇𝑖 . For instance, if a user wants to determine the maximum stock price of a company at a specific point in time, they may ask a question such as, “What is the stock price of Apple on Jan 1, 2022?"

  We categorize visualization manipulations based on the extent of changes made to the elements, taking into account factors including whether the elements’ positions are changed, whether elements are added or removed, and whether the current encoding method is maintained. We summarized the visualization manipulations into four levels and seven types, as depicted in Figure 6. Here are the four levels of visualization manipulations:

  A few participants have expressed that natural language may not always be the most efficient way to convey certain queries, especially those related to precise timing. Descriptions in natural language can be lengthy and may face issues with machine parsing inaccuracies. Additionally, some interactions cannot be easily represented using the traditional Window, Icon, Menu, and Pointer (WIMP) interface. To address these limitations, we plan to combine the strengths of both approaches and develop efficient multi-modal interaction systems. Natural language can also be used as a programming language for traditional interfaces, allowing complex interactions to be mapped to natural language using simple languages.
